{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b81ae8c",
   "metadata": {},
   "source": [
    "### 순서\n",
    "1. 문서의 내용을 읽는다.\n",
    "2. 문서를 쪼갠다.\n",
    "    - 토큰수 초과로 답변을 생성하지 못할 수 있고\n",
    "    - 문서가 길면(입력이 길면) 답변 생성이 오래걸림\n",
    "3. 임베딩 -> 벡터 데이터베이스에 저장\n",
    "4. 질문이 있을 때, 벡터 데이터베이스에 유사도 검색\n",
    "5. 유사도 검색으로 가져온 문서를 LLM에 질문과 함께 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU  docx2txt langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbcf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5d3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "loader = Docx2txtLoader(\"./tax.docx\")\n",
    "document = loader.load() # chunking 없이 한번에 불러옴\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter) # chunking 하여 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document) # chunking 없이 한번에 불러옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document_list) # chunking 하여 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18afe21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d252b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c0fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# database = Chroma.from_documents(documents=document_list, embedding=embedding, collection_name=\"chroma_tax\", persist_directory=\"./chroma\")\n",
    "database = Chroma(embedding_function=embedding, collection_name=\"chroma_tax\", persist_directory=\"./chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d394c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"연봉 5천만원인 직장인의 소득세는 얼마인가요?\"\n",
    "# retrieved_docs = database.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03178a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87934bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"\"\"[identity]\n",
    "# - 당신은 최그의 한국 소득세 전문가입니다\n",
    "# - [context]를 참고해서 사용자의 질문에 답변해주세요\n",
    "\n",
    "# [context]\n",
    "# {retrieved_docs}\n",
    "\n",
    "# question: {query}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_message = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client(api_key=LANGSMITH_API_KEY)\n",
    "\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e2c6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda0806",
   "metadata": {},
   "source": [
    "# 여기서부터 중단.\n",
    "\n",
    "(문제1-해결)hub이 deprecate 됐고 langsmith를 대체로 활용해야하는데 문서가 복잡해 일단 hub를 왜 쓰는지에 대한 다음 강의 수강후 다시 돌아와 문서 공부 필요.  \n",
    "\n",
    "\n",
    "(문제2)RetrievalQA 가 deprecate 됐는데 이게 뭘 하는 건지, 그리고 뭔지 모르니 뭘로 대체해야 할지도 모르는 상태.\n",
    "\n",
    "관련 검색:\n",
    "- 검색어 RetrievalQA\n",
    "- 검색어 RetrievalQA.from_chain_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=database.as_retriever(), \n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
